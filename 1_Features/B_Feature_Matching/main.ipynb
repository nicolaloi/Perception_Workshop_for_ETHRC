{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f89d968",
   "metadata": {},
   "source": [
    "# Feature Matching\n",
    "\n",
    "This notebook will guide you through the process of finding corresponding features between two images. This is a fundamental task in computer vision with applications in 3D reconstruction, object recognition, loop closure, and more.\n",
    "\n",
    "We will cover the following steps:\n",
    "2.  **Feature Detection and Extraction**: Use the SIFT (Scale-Invariant Feature Transform) algorithm to detect keypoints and compute their descriptors.\n",
    "3.  **Feature Matching**: Match the detected features between the two images.\n",
    "4.  **Geometric Model Estimation**: Use RANSAC to estimate a geometric transformation (Homography) from the matches.\n",
    "5.  **Visualization**: Visualize the results, including matched features, inliers/outliers, and warped images or epilines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d685d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2 # opencv is a computer vision library\n",
    "import numpy as np # numpy is a library for numerical computing\n",
    "from matplotlib import pyplot as plt # matplotlib is a plotting library\n",
    "import pathlib\n",
    "import tqdm\n",
    "from enum import Enum\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../../')\n",
    "\n",
    "import utils\n",
    "from utils import show_image, show_images, warp_perspective, show_inlier_matches\n",
    "\n",
    "cwd = os.getcwd()\n",
    "feature_1_block_dir = pathlib.Path(os.getcwd()).parent\n",
    "images_path = feature_1_block_dir / 'images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336d5458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "\n",
    "utils.CV2_MAX_IMAGE_HEIGHT = 1080/2  # set a maximum height for cv2 image visualization\n",
    "utils.SHOW_IMAGE_BACKEND = utils.Backend.PLT  # set the default backend for visualization\n",
    "utils.DRAW_MATCH_THICKNESS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ef9d5",
   "metadata": {},
   "source": [
    "Let's load two images containing the same object. We can apply some transformations to the images to make the feature matching more difficult and interesting.\n",
    "\n",
    "*But attention!* If you modify the images too much, it will not be possible anymore to robustly recognize the same common features in the two images. Suggestion: first complete the exercise without touching these transform parameters, and come back later to this cell to experiment with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0055d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME_1 = 'harry_potter_cover.jpg'\n",
    "IMAGE_NAME_2 = 'harry_potter_library.jpg'\n",
    "\n",
    "### PLAY WITH THESE PARAMETERS TO SEE DIFFERENT RESULTS\n",
    "RESIZE_FACTOR_1 = 0.5\n",
    "RESIZE_FACTOR_2 = 1.0\n",
    "\n",
    "BRIGHTNESS_SCALE_ALPHA_1 = 0.8\n",
    "BRIGHTNESS_OFFSET_BETA_1 = 0\n",
    "\n",
    "BRIGHTNESS_SCALE_ALPHA_2 = 1.0\n",
    "BRIGHTNESS_OFFSET_BETA_2 = 0\n",
    "\n",
    "SCALE_1 = 1.2\n",
    "TRANSLATE_X_1 = 10.0\n",
    "TRANSLATE_Y_1 = 30.0\n",
    "SHEAR_X_1 = 100.0\n",
    "SHEAR_Y_1 = -200.0\n",
    "ROTATION_1 = 20.0\n",
    "\n",
    "SCALE_2 = 1.0\n",
    "TRANSLATE_X_2 = 0.0\n",
    "TRANSLATE_Y_2 = 0.0\n",
    "SHEAR_X_2 = 0.0\n",
    "SHEAR_Y_2 = 0.0\n",
    "ROTATION_2 = 0.0\n",
    "#####################################\n",
    "\n",
    "image_path_1 = images_path / IMAGE_NAME_1\n",
    "image_path_2 = images_path / IMAGE_NAME_2\n",
    "\n",
    "input_image_1 = cv2.imread(str(image_path_1))\n",
    "input_image_1 = cv2.resize(input_image_1, (0,0), fx=RESIZE_FACTOR_1, fy=RESIZE_FACTOR_1)\n",
    "input_image_1 = cv2.convertScaleAbs(input_image_1, alpha=BRIGHTNESS_SCALE_ALPHA_1, beta=BRIGHTNESS_OFFSET_BETA_1)\n",
    "input_image_1 = warp_perspective(input_image_1, scale=SCALE_1, tx=TRANSLATE_X_1, ty=TRANSLATE_Y_1,\n",
    "                                 shear_x=SHEAR_X_1, shear_y=SHEAR_Y_1, rot_deg=ROTATION_1)\n",
    "mask_image_1 = np.ones_like(input_image_1, dtype=np.uint8)\n",
    "mask_image_1 = warp_perspective(mask_image_1, scale=SCALE_1, tx=TRANSLATE_X_1, ty=TRANSLATE_Y_1,\n",
    "\t\t\t\t\t\t\t\t shear_x=SHEAR_X_1, shear_y=SHEAR_Y_1, rot_deg=ROTATION_1)\n",
    "\n",
    "\n",
    "input_image_2 = cv2.imread(str(image_path_2))\n",
    "input_image_2 = cv2.resize(input_image_2, (0,0), fx=RESIZE_FACTOR_2, fy=RESIZE_FACTOR_2)\n",
    "input_image_2 = cv2.convertScaleAbs(input_image_2, alpha=BRIGHTNESS_SCALE_ALPHA_2, beta=BRIGHTNESS_OFFSET_BETA_2)\n",
    "input_image_2 = warp_perspective(input_image_2, scale=SCALE_2, tx=TRANSLATE_X_2, ty=TRANSLATE_Y_2,\n",
    "                                 shear_x=SHEAR_X_2, shear_y=SHEAR_Y_2, rot_deg=ROTATION_2)\n",
    "\n",
    "show_images([input_image_1, input_image_2], [\"Input Image 1\", \"Input Image 2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a003c",
   "metadata": {},
   "source": [
    "### SIFT keypoint detection and descriptor extraction\n",
    "\n",
    "This cell detects SIFT keypoints and computes their descriptors for each image. The resulting keypoints and descriptors are used for matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLAY WITH THESE PARAMETERS TO REFINE THE FEATURE DETECTION\n",
    "SIFT_N_FEATURES = None\n",
    "SIFT_N_OCTAVE_LAYERS = 3\n",
    "SIFT_CONTRAST_THRESHOLD = 0.04\n",
    "SIFT_EDGE_THRESHOLD = 10\n",
    "SIFT_SIGMA = 1.6\n",
    "#########################################\n",
    "\n",
    "SIFT_RICH_VISUALIZATION = True\n",
    "SIFT_KEYPOINT_COLOR = None # (B,G,R) color tuple or None for random color per keypoint\n",
    "\n",
    "sift_extractor = cv2.SIFT_create(\n",
    "\tnfeatures=SIFT_N_FEATURES,\n",
    "\tnOctaveLayers=SIFT_N_OCTAVE_LAYERS,\n",
    "\tcontrastThreshold=SIFT_CONTRAST_THRESHOLD,\n",
    "\tedgeThreshold=SIFT_EDGE_THRESHOLD,\n",
    "\tsigma=SIFT_SIGMA\n",
    ")\n",
    "\n",
    "# extract keypoints\n",
    "sift_keypoints_1, sift_descriptors_1 = sift_extractor.detectAndCompute(input_image_1, None)\n",
    "print(f\"Detected {len(sift_keypoints_1)} SIFT keypoints in image 1\")\n",
    "\n",
    "# compute the descriptors at the detected keypoints\n",
    "sift_keypoints_2, sift_descriptors_2 = sift_extractor.detectAndCompute(input_image_2, None)\n",
    "print(f\"Detected {len(sift_keypoints_2)} SIFT keypoints in image 2\")\n",
    "\n",
    "sift_img = input_image_1.copy()\n",
    "sift_viz_flag = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS if SIFT_RICH_VISUALIZATION else 0\n",
    "cv2.drawKeypoints(sift_img, sift_keypoints_1, sift_img, color=SIFT_KEYPOINT_COLOR, flags=sift_viz_flag)\n",
    "\n",
    "sift_img_2 = input_image_2.copy()\n",
    "cv2.drawKeypoints(sift_img_2, sift_keypoints_2, sift_img_2, color=SIFT_KEYPOINT_COLOR, flags=sift_viz_flag)\n",
    "\n",
    "show_images([sift_img, sift_img_2], [\"SIFT Keypoints Image 1\", \"SIFT Keypoints Image 2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab89cae",
   "metadata": {},
   "source": [
    "## Descriptor matching\n",
    "\n",
    "This cell matches descriptors between the two images using the configured matcher (Brute-Force or FLANN) and filters matches using ratio and distance thresholds.\n",
    "\n",
    "Your task:\n",
    "\n",
    "The next three code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b498c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLAY WITH THESE PARAMETERS TO REFINE THE MATCHING\n",
    "K = 2  # number of nearest neighbors to find (K>1 useful to apply Lowe's ratio test)\n",
    "DISTANCE_RATIO_THRESHOLD = 0.9  # Lowe's ratio test threshold to filter ambiguous matches\n",
    "DISTANCE_ABSOLUTE_THRESHOLD = 500  # absolute distance threshold to filter outliers\n",
    "#########################################\n",
    "\n",
    "#TODO: implement Lowe's ratio test function to filter ambiguous matches\n",
    "def TODO_Lowe_ratio_distance_filtering(\n",
    "\t\tmatches: list[tuple[cv2.DMatch, cv2.DMatch]],\n",
    "\t\tdistance_ratio_threshold: float\n",
    "\t) -> list[cv2.DMatch]:\n",
    "\t# Apply Lowe's ratio test on the first two nearest neighbors (they are returned sorted by distance)\n",
    "\tgood_matches = []\n",
    "\tfor m1, m2, *_ in matches:\n",
    "\t\tif True: # TODO: implement the actual test using distance_ratio_threshold\n",
    "\t\t\tgood_matches.append(m1)\n",
    "\treturn good_matches\n",
    "\n",
    "###########################################\n",
    "\n",
    "def matching(sift_descriptors_1, sift_descriptors_2, K, DISTANCE_RATIO_THRESHOLD, DISTANCE_ABSOLUTE_THRESHOLD):\n",
    "\tflann = cv2.FlannBasedMatcher()\n",
    "\tmatches = flann.knnMatch(sift_descriptors_1, sift_descriptors_2, k=K)\n",
    "\n",
    "\t# Apply Lowe's ratio test to filter good matches\n",
    "\tif K == 1:\n",
    "\t\tgood_matches = [m[0] for m in matches if len(m) > 0]  # if K=1, all matches are considered good\n",
    "\telse:\n",
    "\t\tgood_matches = TODO_Lowe_ratio_distance_filtering(matches, DISTANCE_RATIO_THRESHOLD)\n",
    "\n",
    "\t# Further filter matches based on absolute distance threshold\n",
    "\tgood_matches = [m for m in good_matches if m.distance < DISTANCE_ABSOLUTE_THRESHOLD]\n",
    "\n",
    "\treturn good_matches\n",
    "\n",
    "good_matches = matching(sift_descriptors_1, sift_descriptors_2,\n",
    "\tK, DISTANCE_RATIO_THRESHOLD, DISTANCE_ABSOLUTE_THRESHOLD)\n",
    "\n",
    "# Draw matches\n",
    "SIFT_matches = cv2.drawMatches(input_image_1, sift_keypoints_1, input_image_2, sift_keypoints_2,\n",
    "\tgood_matches, None, matchesThickness=utils.DRAW_MATCH_THICKNESS, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "show_image(SIFT_matches, f\"{len(good_matches)} SIFT Matches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a16f840",
   "metadata": {},
   "source": [
    "### Robust geometry estimation (RANSAC)\n",
    "\n",
    "Now let's repeat the previous parth, but this time we will use very bad matching parameters to obtain a lot of outlier matches, and we will use RANSAC applied to an homography geometric model to filter them. You will see that RANSAC can be very good in rejecting outliers, even when the majority of the matches are outliers.\n",
    "\n",
    "Your task:\n",
    "1) Fix the `TODO_find_homography_ransac` function. Check the [OpenCV documentation for the findHomography function](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga4abc2ece9fab9398f2e560d53c8c9780).\n",
    "2) Play with the parameters to improve and refine the outlier rejection result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a84b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLAY WITH THESE PARAMETERS TO REFINE THE MATCHING\n",
    "RANSAC_REPROJ_THRESH = 10.0\n",
    "RANSAC_CONFIDENCE = 0.9\n",
    "RANSAC_MAX_ITERS = 30\n",
    "########################\n",
    "\n",
    "# TODO: Fix the Homography estimation with RANSAC to filter out outlier matches\n",
    "def TODO_find_homography_ransac(\n",
    "\t\tpts1: np.ndarray,\n",
    "\t\tpts2: np.ndarray,\n",
    "\t\treproj_threshold: float,\n",
    "\t\tmax_iters: int,\n",
    "\t\tconfidence: float\n",
    "\t) -> tuple[np.ndarray, np.ndarray]:\n",
    "\t# Estimate Homography using RANSAC to filter out outlier matches\n",
    "\t# Check https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga4abc2ece9fab9398f2e560d53c8c9780\n",
    "\tH = None\n",
    "\tmask = None\n",
    "\t# H, mask = cv2.findHomography(...)\n",
    "\n",
    "\tif H is None:\n",
    "\t\traise RuntimeError(\"Homography estimation failed. Try to run again (Ransac is random!), or adjust parameters.\")\n",
    "\n",
    "\treturn H, mask\n",
    "\n",
    "########################################\n",
    "\n",
    "# previous function to find matches. Let's use very bad parameters to have many outliers\n",
    "bad_K = 1\n",
    "bad_distance_ratio_threshold = 1.0\n",
    "bad_distance_absolute_threshold = 1000\n",
    "good_matches = matching(sift_descriptors_1, sift_descriptors_2,\n",
    "\tbad_K, bad_distance_ratio_threshold, bad_distance_absolute_threshold)\n",
    "\n",
    "# Draw matches\n",
    "SIFT_matches = cv2.drawMatches(input_image_1, sift_keypoints_1, input_image_2, sift_keypoints_2,\n",
    "\tgood_matches, None, matchesThickness=utils.DRAW_MATCH_THICKNESS, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "show_image(SIFT_matches, f\"{len(good_matches)} SIFT Matches\")\n",
    "\n",
    "\n",
    "# NEW PART: use RANSAC to estimate a Homography and filter out outlier matches\n",
    "\n",
    "if len(good_matches) == 0:\n",
    "\traise RuntimeError(\"No matches to estimate transformation.\")\n",
    "\n",
    "pts1 = np.float32([sift_keypoints_1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "pts2 = np.float32([sift_keypoints_2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "H, mask = TODO_find_homography_ransac(pts1, pts2, RANSAC_REPROJ_THRESH, RANSAC_MAX_ITERS, RANSAC_CONFIDENCE)\n",
    "\n",
    "inlier_mask = (mask.ravel() == 1)\n",
    "\n",
    "print(f\"Homography Matrix estimated with {int(np.count_nonzero(inlier_mask))}/{len(good_matches)} inliers:\\n{H}\")\n",
    "\n",
    "# visualize matches: green = inlier, red = outlier\n",
    "show_inlier_matches(input_image_1, sift_keypoints_1, input_image_2, sift_keypoints_2, good_matches, inlier_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb0c73d",
   "metadata": {},
   "source": [
    "### Warp image\n",
    "\n",
    "Now we can visualize the quality of the estimated homography matrix. If the first image is correctly warped onto the second image’s viewpoint, it means the keypoint matching and outlier rejection were successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb387dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST RUN THIS CELL\n",
    "\n",
    "if H is None:\n",
    "    raise RuntimeError(\"Homography estimation failed. Try to run again (Ransac is random!), or adjust parameters.\")\n",
    "\n",
    "height, width, _ = input_image_2.shape\n",
    "warped_image_1 = cv2.warpPerspective(input_image_1, H, (width, height))\n",
    "mask_image_1 = cv2.warpPerspective(mask_image_1.astype(np.float32), H, (width, height)).astype(bool)\n",
    "show_images([warped_image_1, input_image_2], [\"Warped Image 1 using Homography\", \"Input Image 2\"])\n",
    "\n",
    "merged_image = input_image_2.copy()\n",
    "merged_image[mask_image_1] = warped_image_1[mask_image_1]\n",
    "show_image(merged_image, \"Merged Image using Homography\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0927fbff",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we implemented a feature matching pipeline and explored how local features enable geometric reasoning between images. We have seen how to:\n",
    "\n",
    "- Match descriptors using nearest-neighbor search (FLANN) and filter ambiguous matches with Lowe’s ratio test and distance thresholds.\n",
    "- Use RANSAC to robustly estimate a homography and reject outlier correspondences.\n",
    "- Validate the estimated model by visualizing inlier/outlier matches, warping one image onto the other to inspect alignment.\n",
    "\n",
    "\n",
    "***********************************************************************\n",
    "### IMPORTANT OVERVIEW\n",
    "\n",
    "This simple exercise highlights the broader power of feature detection and matching. **These techniques are not only useful for identifying objects or regions of interest within images, but they also play a crucial role in estimating movement, transformations, and spatial relationships between frames.** By tracking how features shift from one image to another, we can infer camera motion, object motion, scene geometry, and even build consistent representations of the world across multiple views.\n",
    "\n",
    "\n",
    "In other words, feature correspondences form the foundation for tasks like motion estimation, image stitching, 3D reconstruction, and simultaneous localization and mapping (SLAM). The reliability of the homography you see here is just one example of how these underlying methods enable robust geometric understanding in computer vision systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
